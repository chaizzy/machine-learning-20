{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe5ab23-f22a-4be5-94ab-a8d2fb47bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# RandomForestRegressor\n",
    "# It comes under Ensemlbe technique\n",
    "# Random Forest Regressor is a supervised learning algorithm that belongs to the family of ensemble methods. \n",
    "# It is primarily used for regression tasks.\n",
    "# where the goal is to predict a continuous target variable based on a set of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f5df51-2929-4456-bf40-94a9df09b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2:\n",
    "# 1. Bootstrapping: Bagging starts by creating multiple random subsets of the original training data through a process called bootstrapping. \n",
    "# Bootstrapping involves randomly sampling the training data with replacement\n",
    "# 2.Training Multiple Models: After creating the bootstrapped subsets, \n",
    "#    models are trained on each of these subsets independently\n",
    "# 3.Aggregation: Once all  are trained, bagging combines their predictions to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c11424-797c-4a53-a510-b76b032988e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3:\n",
    "# 1.Ensemble of Decision Trees: Random Forest Regressor creates an ensemble of decision trees.\n",
    "#  Each decision tree is constructed using a random subset of the training data and a random subset of features at each node. \n",
    "# 2.Prediction from Individual Trees: Once the decision trees are trained, they can make predictions on unseen data. \n",
    "# 3.Aggregation of Predictions: To obtain the final prediction from the Random Forest Regressor,\n",
    "#   the predictions of all the individual decision trees are combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ec66dd-426d-4ea5-8496-c48b27a51578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4:\n",
    "# Hyperparameters\n",
    "# 1.n_estimators: It specifies the number of decision trees in the random forest.\n",
    "#  Increasing the number of trees generally improves performance, but it also increases computational complexity.\n",
    "# 2.criterion: This hyperparameter determines the quality of a split in each decision tree. It can be set to \"mse\" (mean squared error) or \"mae\" (mean absolute error). \n",
    "#  \"mse\" is the default option and is commonly used for regression tasks.\n",
    "# 3.max_depth: It defines the maximum depth of each decision tree in the forest. \n",
    "#   Setting a higher value can result in more complex trees, which may lead to overfitting.\n",
    "# 4. min_samples_split: This parameter specifies the minimum number of samples required to split an internal node during tree construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82932aab-b6aa-40a8-944b-93fe673c6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5:\n",
    "# The main difference between Random Forest Regressor and Decision Tree Regressor lies in their approach to modeling. \n",
    "# Random Forest Regressor is an ensemble method that combines multiple decision trees, \n",
    "# whereas Decision Tree Regressor is a single decision tree model.\n",
    "# Random Forest Regressor tends to have higher predictive power compared to a single Decision Tree Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef26a74c-ba31-40dd-b5fd-2a4ed07a3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6:\n",
    "# Advantages\n",
    "# 1.Easy to understand and interpret.\n",
    "# 2.Can handle both numerical and categorical features.\n",
    "# 3.Nonlinear relationships can be captured through tree branching.\n",
    "# 4.Robust to outliers.\n",
    "# 5.Reduces overfitting and variance compared to a single decision tree.\n",
    "\n",
    "# Disadventages\n",
    "# 1.Prone to overfitting if the trees are too deep or complex.\n",
    "# 2.May not capture certain types of relationships well, such as linear relationships.\n",
    "# 3.Sensitive to noisy or mislabeled data, which can lead to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b14166-4a88-42a7-b031-3ff8d8d8beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7:\n",
    "# The output of Random Forest Regressor is a prediction of the continuous target variable for the given input features. \n",
    "# It provides an estimated numerical value as the regression output.\n",
    "# the final prediction is obtained by taking the average of the predictions from all the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db996090-9482-4c06-83c1-b336908d495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 8:\n",
    "# Yes, Random Forest Regressor can be adapted for classification tasks as well.\n",
    "# While Random Forest Regressor is primarily designed for regression problems, \n",
    "# it can be utilized for classification by applying a few modifications.\n",
    "\n",
    "# By following ways we can in classification tasks also\n",
    "# For a binary classification problem with two classes (e.g., classifying emails as spam or not spam), \n",
    "# you can modify the target variable by encoding the classes as 0 and 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
